# 주문 개선하기: 재고 업데이트 안정화

2024년 2분기부터 시작한 주문 개선하기 중 두 번째로 작업했던 재고 업데이트 안정화 작업에 대해서 적어보도록 하겠습니다.

## 이전 글
- [1.대량 상품 출고 데이터 처리](https://github.com/ejoongseok/blog/blob/main/%EA%B0%9C%EB%B0%9C%EC%9D%BC%EC%A7%80/1.%EB%8C%80%EB%9F%89%20%EC%83%81%ED%92%88%20%EC%B6%9C%EA%B3%A0%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%B2%98%EB%A6%AC.md)
- [2.서비스간 의존성 격리: 트랜잭셔널 아웃박스 패턴](https://github.com/ejoongseok/blog/blob/main/%EA%B0%9C%EB%B0%9C%EC%9D%BC%EC%A7%80/2.%EC%84%9C%EB%B9%84%EC%8A%A4%EA%B0%84%20%EC%9D%98%EC%A1%B4%EC%84%B1%20%EA%B2%A9%EB%A6%AC%3A%20%ED%8A%B8%EB%9E%9C%EC%9E%AD%EC%85%94%EB%84%90%20%EC%95%84%EC%9B%83%EB%B0%95%EC%8A%A4%20%ED%8C%A8%ED%84%B4.md)
- [3.주문 개선하기: 시퀀스 테이블 페이드아웃](https://github.com/ejoongseok/blog/blob/main/%EA%B0%9C%EB%B0%9C%EC%9D%BC%EC%A7%80/3.%EC%A3%BC%EB%AC%B8%20%EA%B0%9C%EC%84%A0%ED%95%98%EA%B8%B0%3A%20%EC%8B%9C%ED%80%80%EC%8A%A4%20%ED%85%8C%EC%9D%B4%EB%B8%94%20%ED%8E%98%EC%9D%B4%EB%93%9C%EC%95%84%EC%9B%83.md)

## 문제 상황
지난해부터 지금까지 케타포의 물류 시스템과 커머스 시스템은 정말 많은 성장을 해왔다.   
내가 처음 입사했던 22년 8월에는 상상도 못 했을 만큼 서비스는 많이 개선되고 안정화되었다.   

하지만 여전히 상품의 재고는 많은 곳에서 변경이 일어나고 있었고, 트래픽이 몰리면 문제가 될 수 있었다.  
지금까지의 경험에 따르면 업데이트에 부하가 걸리면 성능에 문제가 생기고 결국 **장애**로 이어졌다.   

![image](https://github.com/user-attachments/assets/c7450f90-2e64-4f71-a4c2-e063b6e73dcf)

주문 시 상품의 재고가 변경되기 때문에, 주문에 부하가 발생하면 문제가 생길 것을 예상할 수 있었다.  

케타포는 유명 아티스트의 새 앨범 발매나 홍보 등 마케팅 이벤트가 발생하면 평소보다 **수십 배** 많은 트래픽이 유입되는 특징이 있었다.    
![image](https://github.com/user-attachments/assets/55f8c44a-6bcb-4b1c-a6fc-7c73480ce446)

상품 재고는 시스템 전반에서 의존하고 있는 [**단일 장애 지점(SPOF)**](https://ko.wikipedia.org/wiki/%EB%8B%A8%EC%9D%BC_%EC%9E%A5%EC%95%A0%EC%A0%90)이었다.  

재고는 RDB로 관리되고 있었는데, 리더 DB가 1개인 리더 기반 복제 구조로 DB를 사용하기 때문에 이벤트 상황에서의 **스케일 아웃**으로는 재고 변경 트래픽을 대응할 수 없었다.   
재고의 변경이 많이 일어나는 때에는 **dead lock**, **lock wait time out**이 발생하고 있었다.  

우리는 이 문제를 해결하기 위해 재고 변경 프로세스를 개선해야 했다.     

## 개선 방법
- 참고: 실제로는 여러 태스크가 병렬로 진행되고 묶여서 진행된 작업도 있었지만, 이해하기 쉽도록 순서를 조정하고 풀어서 작성했습니다.
### 단일 진실 공급원(SSOT) 확보
케타포에서 상품의 재고는 여러 서비스에서 각각의 이유로 직접 변경되고 있었다.  
이러한 재고를 개선하는 일은 전체 서비스에 큰 영향을 미치기 때문에,  
우선 변경이 한 곳을 통해서만 이루어질 수 있도록 **단일 진실 공급원**을 만들어야 했다.  

#### BEFORE
![image](https://github.com/user-attachments/assets/3466ebe1-0133-4714-a5df-9f72a9beccba)

#### AFTER
![image](https://github.com/user-attachments/assets/0bb1295f-2407-4c00-bf99-8de2289d5ddd)


문제는 변경이 여러 서비스에서 이루어지는 구조인 만큼, 개선이 누락된 부분이 있을 수 있다는 점이다.   
따라서 모든 재고 변경이 **SSOT를 통해 이루어지는지 확인**해야 했다.

#### Dual Write를 통한 데이터 검증
놓친 부분이 없는지 확인하기 위해 기존 재고 변경 로직에 SSOT인 재고 서비스를 호출하는 코드를 추가했다.  


``` java
재고 업데이트() {
  기존 재고 변경 로직();
  재고 서비스 호출();
}
```

이후 검증용 신규 재고 테이블을 만들고, 기존 재고를 모두 복사하여 **Dual Write**를 진행했다.   
재고가 변경되는 부분을 모두 파악한 것이 맞다면 **두 테이블의 재고는 항상 일치**해야 했다.  

![image](https://github.com/user-attachments/assets/cb182d12-cb34-4163-999c-7d9c33122747)
  
안타깝지만 모니터링 결과 재고 수량에 차이가 발생했고, 이를 보완하는 데 거의 2달이 걸렸다.    
이 부분에 대해서는 뒤에서 좀 더 자세히 설명하려고 한다.  

#### 재고 변경 로직 이관 준비

다음으로는 각 서비스에서 변경되던 재고가 재고 서비스를 통해서만 변경될 수 있도록,   
기존 재고 변경 로직을 재고 서비스로 **lift & shift** 방식으로 복사한 뒤, **feature flag**를 추가해 이관을 준비하는 것이었다.  
``` java
flag off
재고 업데이트() {
  기존 재고 변경 로직(); <- 동작함
  재고 서비스 호출();
}
--- 
flag on
재고 업데이트() {
  기존 재고 변경 로직(); <- 동작안함
  재고 서비스 호출(); <- 재고 서비스에 lift & shift로 이관한 기존 재고 변경 로직이 동작.
}

```

하지만 재고 서비스로 재고 변경을 이관하더라도 성능적으로는 아직 개선된 부분이 없었다.    
여전히 주문 트래픽이 몰리면 재고 서비스도 그만큼 많이 호출될 것이고, 재고 업데이트에도 부하가 발생하는 상황이었다.  

### RateLimiter 적용

부하가 발생하는 상황에서도 재고 서비스가 안정적으로 재고를 업데이트할 수 있도록 **RateLimiter**를 적용했다.  

그러나 트래픽을 제어하다 보니 최종 일관성이 맞춰지기까지 지연이 생기기 시작했다.  

**K6**로 부하 테스트를 진행한 결과, **1분간 300VUs(대략 15,000~18,000건)** 기준,   
RateLimiter를 통해 **초당 45건**씩 처리할 경우 주문이 완료되고 재고의 최종 일관성이 맞춰지기까지 **5분**이 소요되는 것을 확인할 수 있었다.  

이렇게 되면 실제로 상품의 재고가 모두 소진되더라도 재고 수량 변경 반영이 늦기 때문에 **상품이 품절되지 않고 재고 수량을 초과하여 주문할 수 있는 문제**가 생긴다.  
이 문제를 해결하기 위해 최종 일관성이 맞춰지는 시간을 줄일 방법이 필요했다.   
RateLimiter를 좀 더 여유롭게 조절하는 방법도 있었지만, 트래픽이 늘어나면 똑같이 지연되는 문제가 발생하기 때문에 **근본적인 해결책**이 필요했다.  

### Event Sourcing Pattern
근본적인 문제를 개선하기 위해서는 **DB 업데이트 자체를 줄일 방법**이 필요했다.      
논의 결과, **이벤트 소싱 패턴**이 해당 문제를 해결하기에 적합하다고 판단하여 진행했다.  
- [이벤트 소싱 패턴](https://learn.microsoft.com/ko-kr/azure/architecture/patterns/event-sourcing)  

이벤트 소싱 패턴은 업데이트를 줄일 수 있을 뿐만 아니라, 이벤트만으로 재고의 변경 사항을 추적할 수 있다는 장점이 있었다.  

이벤트 소싱 패턴 자체는 단순했기 때문에 적용 전략은 아래와 같았다.
- 재고 서비스는 요청받은 변경 수량을 업데이트하지 않고 이벤트 스토어에 저장한다.**(append only)**
- 스케줄러가 특정 시간마다 **이벤트 스토어를 집계(group by sum)하여 재고를 업데이트**한다.
  - 스케쥴러가 **중복 실행되지 않도록** [shedLock](https://github.com/lukas-krecan/ShedLock)을 사용했다.  
 
![image](https://github.com/user-attachments/assets/f70770e1-41b8-49a8-8a67-5a0ed6a2cc1b)

이벤트 소싱 패턴을 적용한 후, 재고 업데이트는 스케줄러에서만 이루어졌기 때문에 업데이트 쿼리의 발생 횟수를 확연히 줄일 수 있었다.   
또한, 재고 서비스는 단순히 이벤트를 저장만 하면 되므로 **RateLimiter 없이도 트래픽을 무리 없이 처리**할 수 있게 되었다.  
결과적으로 **K6**로 **300VUs 기준 1분**간 테스트했을 때, 이전 성능 대비 **70% 이상 개선**할 수 있었다. (최종 일관성이 맞춰지기까지 5분 -> 1분 안팎)    
추가로, 트래픽 급증이 서비스에 미치는 영향을 최소화할 수 있는 [큐 기반 로드 레벨링 패턴](https://learn.microsoft.com/en-us/azure/architecture/patterns/queue-based-load-leveling)의 이점도 얻을 수 있었다.  

![image](https://github.com/user-attachments/assets/a9d4c4c7-587f-4e47-83c6-cec878b3a8f5)

이미지 출처: https://learn.microsoft.com/en-us/azure/architecture/patterns/queue-based-load-leveling

하지만 이벤트 소싱 패턴을 실제로 사용하기에는 아직 해결하지 못한 **엣지 케이스**가 남아있었다.  

### 엣지 케이스

#### 재고 재할당 (reassign) 이슈
이벤트 소싱 패턴을 실제로 활용하기 위해서는 재고의 DELTA가 아닌 **재고를 직접 SET하는 부분**을 제거해야 했다.  
``` SQL
1. UPDATE 재고 = 파라미터(델타) + 현재 재고
2. UPDATE 재고 = 파라미터 <- 문제 되는 부분
```

하지만 현실적으로 당장 사용되고 있는 재고 재할당 기능을 없앨 수는 없었기 때문에 **차선책**을 선택해야 했다.

재고 서비스로 재할당 요청이 들어오면 **(덮어쓰려는 값 - 현재값)으로 델타를 계산**해서 이벤트를 저장하도록 우회했다.  
![image](https://github.com/user-attachments/assets/927c33be-034b-4b2f-a965-1ebd108816bb)

모든 재고 변경 요청을 델타 이벤트로 저장하도록 변경하면서 이벤트 소싱 패턴을 적용할 수 있게 되었다.  

그런데 이벤트 소싱 패턴을 적용하여 모니터링하던 중, 시간이 지나면서 **간헐적으로** Dual Write 중인 검증용 재고가 틀어지는 문제가 발생하기 시작했다.  

#### 실시간 -> 준실시간 업데이트로 인한 동시성 이슈

원인은 재고 재할당을 델타 이벤트로 변경하고, 재고를 기존처럼 요청별로 바로 업데이트 하는게 아닌,   
재고 변경 이벤트를 저장한 뒤 스케쥴러가 합쳐서 업데이트 하는 **준실시간 방식으로 변경하면서 발생한 이슈**였다.   
- 예시)
- 기존 레거시 동작 방식
  - a-1. 관리자는 재고가 20개 남아있는것을 확인하고 재고를 15개로 변경하려고 한다.  
    b-1. 관리자가 변경 버튼을 클릭하기 직전에 고객이 먼저 상품 10개를 주문한다.  
    b-2. 주문에 의해 재고는 10이 된다.  
    a-2. 거의 동시에 관리자는 재고를 15개로 변경 요청한다.    
    a-3. 관리자의 요청을 처리해 재고는 15가 된다.


| 단계  | 관리자                               | 고객                    | 현재 재고 |
|-------|--------------------------------------|-------------------------|----------|
| a-1   | 재고를 15로 변경 준비                |                         | 20       |
| b-1   |                                      | 상품 10개를 주문        | 20       |
| b-2   |                                      | 주문에 의해 재고를 변경 | 10       |
| a-2   | 재고를 15로 변경 요청                |                         | 10       |
| a-3   | 재고 변경 완료                       |                         | 15       |



- 이벤트 소싱 패턴 적용 후 동작방식
  - a-1. 관리자는 재고가 20개 남아있을때 재고를 15개로 변경하려고 한다.  
    b-1. 관리자가 변경 버튼을 클릭하기 직전에 고객이 먼저 상품 10개를 주문한다.  
    b-2. 주문에 의해 재고 서비스는 -10 재고 이벤트를 저장한다.  
    a-2. 거의 동시에 관리자는 재고를 15개로 변경 요청한다.    
    a-3. 재고 서비스는 15(관리자가 변경하려는재고) - 20(아직 업데이트 되지 않은 현재 재고)로 델타(-5)를 계산해서 이벤트를 저장한다.    
    b-3. 생성된 이벤트를 스케쥴러가 애그리게이션(-10 + -5) 해서 업데이트한다.     
    b-4. 최종적으로 재고는 5가된다.  

| 단계  | 관리자                                              | 고객                           | 현재 재고 |
|-------|-----------------------------------------------------|--------------------------------|----------|
| a-1   | 재고를 15로 변경 준비                               |                                | 20       |
| b-1   |                                                     | 상품 10개를 주문               | 20       |
| b-2   |                                                     | -10 재고 이벤트를 저장         | 20       |
| a-2   | 재고를 15로 변경 요청                               |                                | 20       |
| a-3   | -5 재고 이벤트를 저장 (15 - 20의 결과)              |                                | 20       |
| b-3   |                                                     | 스케줄러가 애그리게이션 시작  | 20       |
| b-4   |                                                     | 재고 변경 완료                 | 5        |


위와 같은 문제를 해결하기 위해 높은 성능과 순서를 보장하며 실시간 반영이 가능한 **인메모리 재고**를 도입할 필요가 있었다.  

### 인메모리 재고로 전환
이벤트 소싱 패턴으로 업데이트 부하를 줄이고 더 짧은 시간 안에 최종 일관성을 맞출 수 있게 되었지만,   
재고의 변경은 **순서를 보장하고 실시간으로 반영**될 필요가 있었다.  

#### 인메모리 재고 변경 프로세스
인메모리 재고로 전환 후, 재고 변경 프로세스는 다음과 같아졌다.

- 재고를 변경하기 위해 재고 서비스를 호출하면 재고 서비스는 인메모리 재고를 먼저 변경한다.  
- 인메모리 재고가 변경되면 카프카를 이용해 '재고 변경됨' 메시지를 발행한다.  
- 카프카 리스너가 '재고 변경됨' 메시지를 구독해서 이벤트 스토어에 저장한다.  
- 스케줄러는 이벤트 스토어에 저장된 이벤트를 집계하여 재고를 업데이트한다.  
![image](https://github.com/user-attachments/assets/2d4ecbfa-726d-48f9-ad30-a6ff2c054fd4)

DB 재고 업데이트가 나중에 이루어지는 만큼,   
재고 조회도 DB 재고를 조회하는 것이 아닌 실시간으로 반영되는 인메모리 재고를 조회하도록 변경해야 했다.  

#### 인메모리 재고 조회
어떻게 하면 복잡한 레거시 코드에 최소한의 변경으로 이 문제를 해결할 수 있을까 고민하다가, 기존의 DB 재고를 가져오는 로직은 건드리지 않기로 했다.  

대신, 재고를 사용하는 클래스에 InMemoryQuantity라는 필드를 만들고 기존에 사용 중이던 quantity 필드의 getQuantity 메소드를 수정했다.  
재고 서비스에 문제가 생겼을 때는 다시 DB 재고를 사용할 수 있게 하기 위해 feature flag도 추가로 적용했다.  

아래와 같은 방법이다.

**BEFORE**
``` java
Product getProduct(id) {
  return repository.getBy(id);
}

class Product {
  Long quantity;

  getQuantity() {
    return quantity;
  }
}
```

**AFTER**
``` java

Product getProduct(id) {
  product = repository.getBy(id);
  inMemoryQuantity = stockService.getStock(id);
  product.assignInMemoryQuantity(inMemoryQuantity);
  return product;
}

class StockService {

  InMemoryQuantity getStock(id) {
    if(!featureFlag.isActive()) return InMemoryQuantity.disable();
    quantity = stockClient.get(url, id);
    return InMemoryQuantity.enable(quantity);
  }
}

class Product {

  Long quantity; <- DB 재고
  InMemoryQuantity inMemoryQuantity;

  getQuantity() {
    if(inMemoryQuantity.isEnable()) return inMemoryQuantity.value();
    return quantity;
  }
}
```

인메모리 전환 후 이벤트 소싱 패턴에서 발생했던 이슈도 개선할 수 있었다.  
- 인메모리 재고 전환 후 동작방식
  - a-1. 관리자는 재고가 20개 남아있을때 재고를 15개로 변경하려고 한다.  
    b-1. 관리자가 변경 버튼을 클릭하기 직전에 고객이 먼저 상품을 10개 주문한다.  
    b-2. 재고 서비스는 인메모리 재고를 10만큼 뺀 뒤 카프카 프로듀서로 -10 재고 이벤트를 발행한다.  
    a-2. 거의 동시에 관리자는 재고를 15개로 변경 요청을 한다.  
    a-3. 재고 서비스는 인메모리 재고를 15로 변경함과 동시에 15(관리자가 변경하려는재고) - 10(이전 인메모리 재고)로 델타(+5)를 반환하고(lua script) 카프카 프로듀서는 +5 재고 이벤트를 발행한다.  
    b-3. 발행된 이벤트를 카프카 리스너가 받아서 이벤트 스토어에 저장한다.  
    b-4. 스케쥴러가 이벤트를 애그리게이션(-10 + 5) 해서 업데이트한다.       
    b-4. 최종적으로 재고는 15가된다.      

| 단계  | 관리자                                              | 고객                           | 레디스 재고 | DB 재고 |
|-------|-----------------------------------------------------|--------------------------------|-------------|---------|
| a-1   | 재고를 15로 변경 준비                               |                                | 20          | 20      |
| b-1   |                                                     | 상품 10개를 주문               | 20          | 20      |
| b-2   |                                                     | 인메모리 재고를 10만큼 뺌, -10 재고 이벤트 발행 | 10          | 20      |
| a-2   | 재고를 15로 변경 요청                               |                                | 10          | 20      |
| a-3   | 인메모리 재고를 15로 변경, +5 재고 이벤트 발행 (15 - 10의 결과) |                                | 15          | 20      |
| b-3   |                                                     | 카프카 리스너가 이벤트 스토어에 저장 | 15          | 20      |
| b-4   |                                                     | 스케줄러가 이벤트를 애그리게이션 시작 | 15          | 20      |
| b-5   |                                                     | 재고 변경 완료                 | 15          | 15      |


또한, 인메모리 재고로 전환 후 **K6**로 **30,000VUs 기준** 약 5분간 부하 테스트를 진행했을 때, **무리 없이 실시간**으로 처리할 수 있었다.  

전환 후 한동안 재고가 일치하는 듯 보였지만, 시간이 지나자 다시 검증용 재고가 크게 틀어지는 일이 발생했다.   

### 아직 해결되지 않은 이슈
현재 시스템에는 대량의 재고를 계산하여 변경하는, **복잡도가 높고 수행 시간이 오래 걸리는 기능**이 존재한다.      
틀어진 재고를 추적해보니, 해당 기능을 사용하면서 발행된 일부 재고 변경 이벤트가 **스케줄러 집계에서 누락**되었다.    
당시 해당 기능이 완료되는 데 약 10분 정도 소요된 것으로 추정된다.  
  
특이하게도, 해당 기능을 사용할 때마다 항상 발생하는 문제는 아니었다.  

당시 스케줄러에 문제가 있었는지 확인했지만, 작업이 수행되던 시점에 눈에 띄는 로그나 지표는 보이지 않았다.   
서비스 로그와 지표, DB 지표를 추가로 확인해도 특이사항이 보이지 않았고,   
스케줄러의 동작 방식은 shedLock을 통해 단일 수행을 보장하고 있었기 때문에 동시성 이슈도 아닌 것 같았다.  

더군다나 이전까지는 문제없이 잘 동작하고 있었고, 스케줄러는 완료되는 데 100ms 정도밖에 걸리지 않는 가벼운 작업이어서 스케줄러에서 원인을 찾기 어려웠다.  

**벌크 요청 이슈**

문제가 발생한 기능의 동작 방식은 기능이 모두 수행된 후 데이터 변경 건을 모아서 **재고 서비스를 한 번만 요청**하는 구조였다.  
이유는 마지막에 호출하는 재고 서비스에서 예외가 발생할 경우 트랜잭션이 깨져 쉽게 롤백할 수 있고,  
재고 서비스를 호출하는 빈도가 적어 부하를 낮출 수 있기 때문이었다.  

**재고 업데이트 동작 방식**
``` java
재고 업데이트() {
  List 변경해야할_데이터_목록 = 재고 변경 로직();
  재고 서비스 호출(변경해야할_데이터_목록);
}
```

다음은 저장된 이벤트를 집계하여 재고를 업데이트하는 스케줄러의 동작 방식이다.

**스케쥴러의 동작 방식**

1. 마지막 스케쥴링 후 새롭게 적재된 이벤트 목록 조회
2. 조회 된 수량만큼 재고 업데이트
3. 조회된 목록 중 OFFSET(auto_increment PK)이 가장 높은 값으로 최종 OFFSET 업데이트

SQL로 보면 다음과 같다.

``` sql
1. 마지막 스케쥴링 후 새롭게 적재된 이벤트 목록 조회
SELECT 재고_ID,
       SUM(DELTA) AS total_delta,
       MAX(OFFSET) AS max_offset
FROM 이벤트 스토어 
WHERE OFFSET > LAST_OFFSET 
GROUP BY 재고_ID

- 조회 결과 -
재고_ID | total_delta | max_offset
  1      |  10         | 3
  2      |  50         | 5

- 이벤트 스토어 원본 데이터 -
재고_ID | DELTA | OFFSET
  1      |  3         | 1
  1      |  3         | 2
  1      |  4         | 3
  2      |  20        | 4
  2      |  30        | 5

2. 조회 된 수량만큼 재고 업데이트
UPDATE 재고 테이블
SET QTY = QTY + :total_delta
where 재고_ID = :재고_ID

3. 조회된 목록 중 OFFSET이 가장 높은 값으로 최종 OFFSET 업데이트
UPDATE OFFSET 관리 테이블
SET OFFSET = :max_offset (조회된 목록중 max_offset이 가장 높은값)


재고_ID | total_delta | max_offset
  1      |  10         | 3
  2      |  50         | 5 <- 이 값으로 OFFSET이 업데이트 됨

```

스케쥴러는 계산을 효율적으로 하기 위해 현재 OFFSET보다 높은 이벤트만 조회해서 계산하는 방식으로 동작했다.  
시간 순으로 보면 아래의 표와 같이 재고와 OFFSET이 변경된다.   

버전은 스케쥴러가 한번 수행 될때마다 1씩 올라간다고 가정한다.  
#### 첫번째 스케쥴링
![image](https://github.com/user-attachments/assets/b82ac220-22cd-41f9-b88a-235db9e64e6c)
#### 두번째 스케쥴링
![image](https://github.com/user-attachments/assets/0e17d694-8cc7-409f-a4c1-305cac87f54c)
#### 세번째 스케쥴링
![image](https://github.com/user-attachments/assets/e81f8a66-8bc7-4fb7-bfc9-229651d3c28c)


이때까지는 위와 같은 방식으로 스케줄러가 문제없이 잘 동작하고 있었다.  
그렇다면 왜 이번에는 이벤트 스토어의 일부 데이터만 업데이트에 반영되었을까?    

결론은 저장해야 할 데이터가 너무 많아 **커밋이 늦어지면서** 발생한 문제였다.  
대량의 재고를 계산하여 변경하는 기능은 많은 변경이 발생하는 만큼 저장해야 할 재고 변경 이벤트도 많았기 때문에, 커밋을 완료하는 데 시간이 오래 걸렸다.  

#### 문제가 발생할 수 있는 케이스
아래는 문제가 될 수 있는 상황이다.  
예시: 하나의 트랜잭션이 대량의 데이터를 한번에 저장해서 커밋해야할 때, **중간에 다른 트랜잭션이 이벤트를 저장**해서 커밋하는 경우 문제가 발생한다.  

| 트랜잭션 A                      | 트랜잭션 B                      | OFFSET | 스케쥴러                                 | 이벤트 스토어 rows  | LAST OFFSET  |
| ------------------------------- | -------------------------------- | -------------- | ---------------------------------------- | ------------------- | ------------------- | 
| 20000개의 데이터 저장 시작 |                                  |     0    |                                          | 0                   | 0 |
| 1~10000개 까지 이벤트 INSERT 진행 (커밋 X) |                                  | 1~10000        |                                          | 0                   | 0 |
|                                 | 100개의 이벤트 INSERT (커밋 X)   | 10001~10100    |                                          | 0                   | 0 |
|                                 | 100개의 이벤트 커밋              |                |                                          | 100                 | 0|
|                                 |                                  |                | 이벤트 조회 id > 0                       | 100                 | 0|
|                                 |                                  |                | 100개의 데이터 집계 후 업데이트                  | 100                 | 0|
|                                 |                                  |                | LAST_OFFSET 10100으로 변경                     | 100                 | 10100|
| 나머지 10000개의 이벤트 INSERT (커밋 X) |                                  | 10100~20100        |                                          | 100                   | 10100|
| 20000개의 이벤트 커밋           |                                  |                |                                          | 20100               | 10100|
|                                 |                                  |                | 이벤트 조회 id > 10100                   | 20100               | 10100|
|                                 |                                  |                | 10101~20100 구간의 데이터 집계 후 업데이트                        | 20100               | 10100|
|                                 |                                  |                | LAST_OFFSET 20100으로 변경                     | 20100                 | 20100|

이벤트 스토어의 OFFSET은 **auto_increment** 값이었는데, 트랜잭션의 **커밋 여부와 상관없이 insert를 하면 증가**한다.  
마찬가지로 **트랜잭션이 롤백**되더라도 auto_increment 값은 **줄어들지 않는다**.   
A 트랜잭션에 데이터가 너무 많아 저장을 요청 한 뒤 커밋이 늦어지는 경우,   
B 트랜잭션이 이벤트를 저장하게 되면 B 트랜잭션의 이벤트 OFFSET이 A 트랜잭션에서 저장 중인 이벤트의 OFFSET보다 더 높게 설정되는 것이다.  

#### 개선 방법 
이벤트가 쌓이고 커밋이 늦어지는 문제를 해결하기 위해, 재고 서비스를 마지막에 모아서 한 번 호출하는 것이 아닌 **실시간으로 호출하도록** 변경했다.  
다만, 중간에 재고 서비스가 실패했을 때 DB 트랜잭션은 롤백되지만, 재고 서비스에서 실시간으로 반영된 재고는 **롤백이 불가능했다**.  
그래서 이미 변경된 데이터를 롤백하기 위해 catch 구문에서 지금까지 반영한 재고 수량을 역으로 계산하여 롤백 요청을 할 수 있도록 코드를 추가했다.  

이런 식으로 트랜잭션에 묶이지 않는 타 서비스에 이미 반영된 변경 사항을 롤백하는 것을 MSA에서 [보상 트랜잭션](https://learn.microsoft.com/ko-kr/azure/architecture/patterns/compensating-transaction), [SAGA 패턴](https://learn.microsoft.com/ko-kr/azure/architecture/reference-architectures/saga/saga)이라고 부르는듯 하다.  

``` java
try {
  ...
  for {
    레거시 재고 변경 로직();
    재고 서비스 호출();
  }
  ...
} catch (RuntimeException e) {
   재고 서비스 롤백 호출();
   throw e;
}
```

재고 서비스를 실시간 호출로 변경하면서 더 이상 커밋 시간이 오래 걸려 재고가 틀어지는 이슈는 발생하지 않게 되었다.  

하지만 아쉽게도 시간이 지나면서 다른 이유로 재고가 틀어지는 일이 발생했다.   

그래도 다행히 모니터링 기간에 발견되었다.    

#### graceful shutdown time out 이슈 
이번에 재고가 틀어지게 된 원인은 대량의 재고를 계산하여 변경하는 기능이 동작하던 중 **애플리케이션이 새로 배포**되면서,  
이미 동작하던 기존 애플리케이션이 종료되는 과정에서 **프로세스를 마무리하지 못하고 종료**되어 발생한 문제였다.    

애플리케이션 수준에서 **graceful shutdown 옵션**이 활성화되어 있었지만,   
프로세스가 10분 이상 걸리는 특정 기능은 프로세스가 끝나기를 일정 시간 기다려 주다가,   
설정된 **타임아웃 시간을 초과**하자 프로세스를 **그냥 종료**해버린 것이었다.  

프로세스가 중간에 강제 종료되더라도 DB 트랜잭션은 롤백될 수 있었지만,   
실시간으로 반영되고 있던 인메모리 재고에 대해서는 애플리케이션이 롤백 요청을 보내지 못하면서 재고가 틀어진 것이었다.  

이 문제를 해결하기 위해 기본값으로 설정되어 있던 spring.lifecycle.timeout-per-shutdown-phase의 설정을 수정했다.(기본값 30초)    
다행히 스프링의 해당 설정을 변경하는 것으로 간단히 해결할 수 있었다.  

마침내 더 이상 재고가 틀어지는 일은 발생하지 않게 되었다.  

## 성과 
여러 서비스에서 개별적으로 재고를 변경하며 발생했던 재고 관리의 복잡성을 줄이고, 오류 발생 가능성을 낮췄다.  
유명 아티스트의 이벤트로 트래픽이 몰리더라도 오류없이 재고 업데이트를 안정적으로 처리할 수 있게 되었다.  

## 소감

많은 시행착오가 있었지만, 동료들과 힘을 합쳐 재고 업데이트를 안정화할 수 있었다.  

재고가 틀어질 때마다 놓친 엣지 케이스가 없는지 찾으면서 가설을 세우고 검증했다.   
그리고 발견한 케이스에 대해 해결 방법을 찾고 보완했다.   
보완한 코드를 배포한 뒤 다시 재고를 맞추고 모니터링을 진행했다.  

이번 개선 작업은 짧은 시간에 해결할 수 없는, 긴 시간 동안 꾸준함과 인내를 필요로 하는 작업이었다.  
이전까지 해왔던 것처럼 불완전하지만 우선 빠르게 고객에게 선보이고 피드백을 받아가며 안정화시키고 개선하는 것이 아닌,   
시간이 걸리더라도 실수 없이 안전하게 하는 것이 더욱 중요한 과제였다.  

초반에는 이렇게 오래 걸릴 줄 모르고 재고가 틀어지면 코드를 고친 뒤 따로 기록이나 정리를 하지 않았었다.   
하지만 시간이 지나면서 점점 더 다양한 엣지 케이스가 발견되고 원인을 파악하기 어려워지면서 재고가 틀어질 때마다 기록했다.  

그러다 보니 문서를 작성하는 와중에 문제를 발견하고 해결 방법을 찾는 날도 있었다.  

문서는 대략 아래와 같은 형식으로 작성했다.  

```
인지 시점

최초 발생 예상 시점

배포 여부

추정 원인

특이사항

로그

지표

결론

해결 방법
```

다양한 엣지 케이스를 마주하면서 고생도 많았지만, 배운 것이 훨씬 많았다.  
동시성의 세계는 정말 아주 작은 하나의 스텝도 영겁의 시간처럼 생각하고 접근해야 하는 것 같다.  

만약 처음부터 자신만만하게 기존 재고 변경 로직을 아예 대체하도록 만들었으면 대참사가 일어났을 것이다.  
다행히 안전하게 작업할 수 있는 환경을 만들어주고 격려해주신 팀장님께 감사하다.  

중간에 좌절하는 날도 있었지만, 끝까지 포기하지 않고 문제를 해결하게 되어 뿌듯하다.  
